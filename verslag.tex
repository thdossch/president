\documentclass[11pt]{article}
\usepackage{a4wide}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\graphicspath{{./images/}}
\usepackage[dutch, english]{babel}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{tabularx}
\begin{document}

\begin{titlepage}
\title{Reinforcement learning gebaseerde agent voor presidenten}
\author{Freya Van Speybroek \& Thor Dossche}
\date{Academiejaar 2020 \- 2021}
\maketitle
\thispagestyle{empty}
\end{titlepage}


\section{Inleiding}
In dit project zullen we bestuderen hoe reinforcement learning kan toegepast worden op een kaartspel. Het kaartspel dat we gaan bekijken is presidenten. Dit is een perfect voorbeeld van een spel waarbij we incomplete informatie hebben: er kan namelijk niet in de kaarten van de tegenstanders gekeken worden. Daarbij is elke toestand discreet, waardoor het probleem kan gemodelleerd worden als een Partially Observable Markov Decision Process (POMDP).\\\\
Voor dit soort problemen zijn er al heel wat oplossingsmethodes, waarvan we er een paar zullen uitproberen en vergelijken. Daarbij kunnen we ons ook de vraag stellen of het wel degelijk rendeert om Reinforcement Learning te gebruiken, misschien zijn we beter af met een heuristiek? \\
\subsection{Het probleem}
Presidenten kent veel varianten, de regels liggen namelijk niet hard vast en kunnen daarom vaak besproken worden onder de spelers. Afhankelijk van welke regels je gebruikt, kan het spel moeilijker worden. \\\\
Bij het spelen van presidenten speel je meerdere spelletjes. Na ieder spel zullen de `rangen' voor het volgend spel gekend zijn. Ieder spel bestaat uit meerdere rondes. In deze rondes proberen de spelers zoveel mogelijk kaarten te leggen om zo al hun kaarten uit te spelen. Tijdens een ronde moeten spelers elk om beurt een kaart leggen hoger of gelijk aan de vorige kaart. Het aantal van die kaarten moet hoger of gelijk zijn aan de kaart(en) van de vorige beurt.\\
Als een speler geen kaart(en) meer kan of wil liggen past hij, eenmaal gepast zal hij niet meer aan de beurt komen in de ronde. Als alle spelers op één na gepast hebben is de winnaar van de ronde gekend. Deze speler mag een nieuwe ronde starten met een kaart of kaarten naar keuze. Als een speler al zijn kaarten heeft uitgespeeld krijgt hij een rang toegewezen voor het volgende spel.\\
De persoon die het eerste uitspeelt is de president, de tweede de vice-president.
De persoon die als laatste overblijft is de scum, degene die als voorlaatste uitspeelt is de vice-scum.\\
Als alle rangen gekend zijn start het volgende spel. Na het delen van de kaarten krijgt de president de beste 2 kaarten van de scum, de scum krijgt de 2 slechtste van de president. Hetzelfde geldt voor vice-president en vice-scum, maar die wisselen slechts 1 kaart uit.\\\\
Nog enkele basisregels:\\
- De speler die de ‘klaver 3’ heeft, start de eerste ronde van een spel met deze kaart (of meerdere kaarten van waarde 3 waaronder de `klaver 3'). \\
- Als een speler uit is, dan wordt een nieuwe ronde gestart en mag de speler links van de net uitgespeelde speler beginnen.\\\\
Om het spel interessanter te maken hebben wij nog een paar extra regels toegevoegd:\\
- Als een 7 is gespeeld, dan moet de volgende kaart lager of gelijk zijn aan die 7 ook het aantal kaarten moet gelijk zijn aan het aantal gelegde kaarten.\\
- De kaart 2 is een joker die kan gebruikt worden als hoogste kaart in het spel, of als een extra kaart. Als de joker gelegd wordt samen met een kaart van een andere waarde zal de joker deze waarde aannemen.\\
- De ranking van de kaarten is dus als volgt: 3,4,5,6,7,8,9,10,V,Q,K,A,2\\

\subsection{De environment}
Bij Reinforcement Learning gaan we niet aan de slag met een dataset, maar met een “environment”, die het spel zelf voorstelt. We willen dan een agent bekomen die een zo hoog mogelijke score behaalt in de environment. \\\\
In dit geval, willen we een agent die zo vaak mogelijk of zo lang mogelijk president kan zijn. Maar, de situatie is niet zo zwart-wit. Het is namelijk ook goed zijn om veel vice-president te zijn want dan ben je nog steeds in een voordeelpositie. Goed spelen kan ook zijn dat je met zeer slechte kaarten toch niet scum wordt maar door tactisch te spelen de high-scum rang behaalt. Soms krijg je gewoon kaarten waarmee je gewoon niet kan winnen hoe goed je ook bent, heb je dan slecht gespeeld als je scum wordt? Het is duidelijk niet zo dat dat altijd winnen een goeie maat is om te meten of je goed bent. Er zal dus gezocht moeten worden naar een goed evenwicht van deze verschillende factoren om te bepalen of de agent goed is. \\\\
Bovendien kan de environment ook op veel verschillende manieren bekeken worden. Hoe wordt een state voorgesteld, welke rewards delen we uit, welke acties maken we mogelijk? Dit zijn allemaal deelproblemen op zich, waarvoor we verschillende oplossingen zullen proberen vinden.\\

\subsection{Mogelijke oplossingsmethoden}
Zoals in de inleiding besproken, is ons probleem gemodelleerd als  een Partially Observable Markov Decision Process (POMDP). Hier zijn verschillende oplossingsmethoden voor mogelijk, waaronder Q-learning, DQN en Monte Carlo Tree Search. Het doel van deze oplossingsmethoden en van Reinforcement Learning zelf, is om een agent, gegeven een toestand en een aantal acties, de meest optimale actie te laten kiezen. Hierbij wordt met optimaal bedoeld dat de agent zijn score maximaliseert. Een actie uitvoeren in een bepaalde state, geeft de agent dus een reward (die goed of slecht kan zijn).\\\\ 
Een eerste interessante weg is Q-learning. Alhoewel dit een algoritme is dat we vaak bij simpele spelsituaties gebruikt zien (zoals \cite{simple-qlearning}), zouden we kunnen kijken of dit goede resultaten geeft. Aangezien Q-learning werkt met een Q-table die per state en per actie een score bijhoudt, kan het toch zijn dat dit te beperkt is voor de grote aantal mogelijke spelsituaties in presidenten. \\\\
Het is daarom misschien interessanter om met een DQN of Deep Q-Network te werken, waarmee we kunnen werken met grotere states om zo meer informatie over het spel bij te houden. Een succesvol voorbeeld met neurale netwerken is \cite{nn-paper}, waar ze werken met spel dat een gelijkaardige complexiteit heeft als presidenten. Waarom ze niet met DQN werken, wordt ook besproken, en is interessant om later in dit onderzoek nog eens aan te halen. \\\\
Ten laatste zou Monte Carlo Tree Search een zelfde beperkingen geven als Q-learning, namelijk de grootte van de state. Hoe groot kunnen we de state maken tot we in een situatie komen waar we te veel paden hebben om te simuleren? Hiervoor zijn er wel al oplossingen, waaronder Partially Observable Monte-Carlo Planning (POMCP), wat een aangepaste Monte Carlo tree search is om POMDPs op te lossen \cite{mct-1}. Er bestaan ook andere manieren door Monte Carlo te combineren met andere oplossingen, zoals in \cite{mct-2} of \cite{mct-3}.\\

\section{Methodiek}
Uit de bovenstaande besproken oplossingsmethodes, bespreken we de eerste twee, namelijk Q-learning en DQN. Interessant hieraan is dat de tweede methode, die met een Deep Q-Network werkt, gebaseerd is op het Q-learning van de eerste methode. Ook bekijken we een alternatieve, eenvoudigere oplossingsmethode die aan de hand van heuristieken of vuistregels van presidenten werkt.\\
\subsection{Heuristiek}

Om te toetsen hoe goed de andere oplossingsmethoden zijn, hebben we een goede speler nodig. Hiervoor hebben we een heuristiek geimplementeerd die ongeveer volgt wat echte spelers in het spel zouden doen, namelijk:
\begin{itemize}
	\item Leg altijd de laagste kaart mogelijk. Als je meerdere van die kaart hebt, leg ze allemaal.
	\item Leg enkel een joker als je niet anders kan.
	\item Pas enkel wanneer je niets kan leggen.
\end{itemize}
Een echte speler zal natuurlijk ook rekening houden met details zoals welke kaarten al gelegd zijn doorheen het spel, hoeveel spelers al uitgespeeld zijn, ... Maar we zullen toch zien dat deze heuristiek opvallend goede resultaten behaald. \\\\
Ook hebben we een extra speler geimplementeerd: de random speler. Deze speler kiest een willekeurige zet uit de mogelijke zetten die hij op dat moment kan doen gegeven de laatst gelegde kaart(en) en de kaarten in zijn hand. Hij past enkel als hij niets anders kan, wat hem iets beter maakt dan een pure random speler! Zo hebben we nu een goede en slechtere speler, waarop getest kan worden.\\

\subsection{Q-learning}
\subsubsection{Theorie}
Q-learning helpt ons zoeken naar een zo goed mogelijke policy \textbf{(todo: leg policy uit? zie dqn-pract)}. De letter $Q$ slaat op de functie die gebruikt wordt om, gegeven een state en een actie, de waarde van deze combinatie te vinden. Deze waarde geeft aan hoe goed - of slecht - een bepaalde actie is in een bepaalde state.\\\\
Op basis van de Bellman equations \cite{bellman-equations}, kunnen we een update regel definieren die er als de formule in Figure \ref{fig:qfunction} uitziet. Een Q-waarde van een bepaald state-actie paar zal dus veranderen met de tijd. Daarom moeten we in het begin de Q-waarden initialiseren, om die dan wanneer nodig aan te passen.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.70]{images/qformula.png}
\caption{Q function - Source: wikipedia.org/wiki/Q-learning}
\label{fig:qfunction}
\end{figure} \\
Er zijn nog een aantal belangrijke parameters aanwezig in de formule. 
\begin{itemize}
	\item Learning rate: Geeft aan in welke mate we de Q-waarde updaten, een grote learning rate zal dus meer impact hebben op de uiteindelijke Q-waarde dan een kleinere.
	\item Reward: Score die berekend wordt op basis van de state waar we in waren en de gekozen actie. Door deze actie uit te voeren, komen we in een nieuwe state en kunnen we kijken hoe waardevol deze actie was. 
	\item Discount factor: Geeft aan hoe belangrijk toekomstige rewards zijn. Een hoge discount factor zal meer belang hechten aan rewards over een lange tijd, een lage discount factor laat het algoritme enkel rewards in een korte tijdsperiode beschouwen.
	\item Estimate of optimal future value: In deze stap gaan we de Q-waarden overlopen over alle mogelijke acties in de volgende state, en halen we de hoogste Q-waarde eruit. Merk op dat als de discount factor 0 is, dit niet uitmaakt, en dus niet naar toekomstige states of rewards kijken.
\end{itemize}
Daarbij is het in Q-learning belangrijk om te beschouwen hoe we de volgende actie a kiezen, om dan overeenkomende $Q(s,a)$ aan te passen. We zouden elke keer de beste actie kunnen kiezen op basis van de Q-waardem, maar dit zal in begin niet werken, aangezien er dan nog helemaal niets geleerd is. We willen dus genoeg exploratie hebben, zeker in het begin! Een oplossing hiervoor is om een $\epsilon$-greedy policy te volgen. Waarbij we met kans $\epsilon$ een willekeurige actie kiezen, en met kans $1-\epsilon$ een actie kiezen gebaseerd op de huidige policy. We kunnen deze $\epsilon$ eventueel laten variëren met de tijd.\\\\

\subsubsection{Implementatie}
De Q-waarden die doorheen het algoritme geüpdate worden, worden bijgehouden in een Q-table. In onze eerste implementatie houdt de state 2 velden bij die de laatst gelegde kaart en zijn hoeveelheid voorstellen. De state wordt gegeven als een tuple (rang, aantal), en acties worden op dezelfde manier voorgesteld. Zo indexeren we de Q-table met deze 2 tuples. \\\\
We kunnen natuurlijk bediscussiëren of het niet beter zou zijn om een grotere state bij te houden, maar hier komen we later bij onze testresultaten op terug. Er is wel een beperking in hoe groot we de state kunnen maken, aangezien er een punt zou zijn waar de tabel zo groot wordt dat veel states niet meer bezocht worden. \\\\
De initiele Q-waarden in de tabel zijn 0 voor alle acties behalve `skip'. Vanaf nu zullen we het over een skip hebben als het over een beurt passen gaat. De waarden voor skip in elke mogelijke state wordt op -1 gezet, wat betekent dat we die al in het begin afstraffen. Op deze manier proberen we ervoor te zorgen dat de agent zo veel mogelijk beslist om wel een kaart te leggen. \\\\
Aangezien we hier de state beperkt moeten houden, moeten we best ook de taktiek van de agent beperkt houden, door hem op zo een goed mogelijke manier de rondes te laten spelen spelen, en niet te kijken naar winnen van het spel. Natuurlijk zal je het spel vaak winnen als je goed speelt in rondes. Dit is ook wat onze heuristieke speler doet. Welke reward functie we daaroor implementeren zullen we bespreken bij resultaten.\\\\
Voor de $\epsilon$ kiezen we als startwaarde 1, en werken we met epsilon decay. Na iedere actie zullen we de huidige $\epsilon$ vermenigvuldigen met een $epsilon\_decay$ parameter. Op deze manier is er in het begin van het trainen meer exploratie maar naarmate de tijd vordert zal $\epsilon$ afnemen en zullen we meer exploitatie hebben. We zullen echter wel zorgen dat eenmaal $\epsilon == 0.05$ de waarde van $\epsilon$ niet meer veranderd. Dit omdat we toch steeds wat exploratie willen hebben. In appendix \ref{appendix:epsilon} bespreken we hoe best deze $\epsilon$ te kiezen.\\\\
Ook hebben we een poging gedaan om de state-acties in een bitvector vorm te beschrijven. Dit omdat als we met grotere states en acties werken we zo meer geheugen zouden besparen dan gebruik te maken van tuples. Echter bleek dat dit negatief was, ook voor meer uitgebreide state-actie representaties, voor de performantie van het trainen en geen significante verbeteringen bijbracht in de prestatie van de agent. Daarom zijn we van dit idee afgestapt.\\

\subsection{Deep Q-Network}
\subsubsection{Theorie}
In plaats van de tabel letterlijk op te slaan en up te daten, benaderen we in het DQN-algoritme de tabel met een functie-approximator. Deze functie-approximator is een neuraal netwerk.\\\\
Het updaten van dit neuraal netwerk werkt dan als volgt. Stel we bevinden ons in een toestand $s$, dan wordt er een actie $a$ gekozen op basis van een $\epsilon$-greedy policy. Door deze actie uit te voeren, komen we in een nieuwe state $s'$ terecht en krijgen we ook een reward $r$. Waar we bij Q-learning hiermee de Q-table kunnen updaten, is het hier niet zo simpel. \\\\
De volgende stap is om deze originele state, de actie, de nieuwe state en de reward op te slaan in een replay memory. In dit replay memory krijgen we dus een verzameling van tuples van de vorm $(s,a,r,s')$. In elke stap nemen we dan samples uit het replay memory en stellen we targets op. Deze targets worden ongeveer dezelfde manier berekend als de temporal difference targets bij Q-learning (zie Figure ~\ref{fig:qfunction}). Enkel is hier het verschil dus dat $Q$ nu een functie-approximator is. Daarbij zien we dat deze targets van de vorm `reward + estimate of optimal future value' zijn. Dit tweede deel is niet meer nodig als we aan het einde van de episode zijn, dan houden we dus enkel de reward over als target. \\\\
Daarna wordt het netwerk geüpdate met de MSE-loss functie, en kunnen we dit allemaal herhalen.\\

\subsubsection{Implementatie}
Gelukkig moeten we hier niet het warm water opnieuw uitvinden, en zijn er al een aantal frameworks ter beschikking die ons een handje kunnen helpen. Wij hebben gekozen voor PyTorch, die dus de implementatie details voor zich neemt.\\\\
Het belangrijkste was hier om te beslissen hoe we precies ons netwerk gingen voorstellen: welke inputs, hidden layers en outputs hebben we nodig?\\\\
Ons initieel idee was om niet te beginnen met een te grote state, om te kijken of we toch resultaten gelijkaardig aan de heuristieke speler konden krijgen met dezelfde informatie. Daarom hebben we gekozen om de de kaarten in de hand bij te houden en de laatst gelegde kaart. De state bevat dan 15 velden. De eerste 13 velden geven aan hoeveel kaarten je van elke waarde in je hand hebt. De laatste 2 tonen dan hoeveel kaarten van welke rang als laatste zijn gelegd (met speciale waarden voor een skip en het begin van een ronde). Om te zorgen dat deze inputs genormaliseerd zijn, gebruiken we volgende formule $norm\_aantal = (aantal - 2)/2$ om het aantal kaarten van een waarde voor te stellen.\\\\
Na de input laag hebben we 2 hidden layers van elk 64 nodes. Daarop volgen de output nodes, namelijk 53 nodes die elk een actie voorstellen. Er is dus een mapping van 0 tot 52 naar een zet die bestaat uit een aantal kaarten van een bepaalde waarde. Deze zet kan ook een skip zijn. Om de output te beperken, kan de agent maar maximum 4 kaarten leggen (ookal is het soms mogelijk om met jokers meer dan dit aantal te leggen).\\

\section{Resultaten}
Om een gevoel te krijgen hoe goed een agent presteert gebruiken we een win/lose ratio. Dit zal enkel rekening houden met hoeveel een agent president is:
\begin{center}
$W/L = games\_president/total\_games\_played * 100$.
\end{center}
We vermenigvuldigen met 100 om het resultaat als percentage uit te drukken. Dit is een goede maat aangezien we hieruit vaak ook kunnen afleiden dat als hij veel president is, hij waarschijnlijk ook vaak vice-president is, en minder scum of high-scum. De maat is echter niet perfect omdat hij afhangt van hoe goed de tegensplers zijn, maar het geeft een goed beeld.

\subsection{Heuristiek en random}
Het is belangrijk om te weten hoe de heuristieke speler het doet tegenover random spelers, en omgekeerd. Zo kunnen we goed vergelijken en hopelijk zelfs streven naar gelijkaardige of betere resultaten. In onderstaande tabel worden de resultaten van simulaties voor deze situatie geïllustreerd.
\begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
                \hline
                  vs                & 3 heuristieke spelers & 3 random spelers \\
                \hline
                 heuristieke speler & 25         & 65\\
                 random spelers     & 6         & 25\\
                \hline
        \end{tabular}
        \caption{W/L in \% voor heuristieke agents}
\end{table}

We zien dat de heuristieke speler het al zeer goed doet, wat wel te verwachten was tegen random spelers. Een goed doel voor de niet-heuristieke agents is dus minstens beter zijn dan de random speler en proberen de heuristieke speler te evenaren, al dan niet te overtreffen.\\

\subsection{Q-learning}
\subsubsection{State met 2 velden}
Zoals beschreven in de implementatiedetails van Q-learning, werken we hier met een state die als (rang, aantal) wordt voorgesteld. Dit is dus een heel beperkte state, waarvoor we een gepaste reward functie moeten vinden. Naast de rewardfunctie kunnen de andere parameters ($\epsilon$, $\alpha$, $\gamma$) ook een invloed hebben.\\\\
We willen, net zoals bij de heuristieke speler, dat de agent een ronde zo goed mogelijk speelt en niet kijkt naar of hij degelijk president is geworden of niet. Dit vertaalt dan naar: de agent moet zo snel mogelijk al zijn kaarten uitspelen, of zo veel mogelijk kaarten spelen per ronde. Hiervoor zijn we tot volgende reward functie gekomen:
\begin{center}
$r = amount\_of\_cards\_played * 0.5 + amount\_of\_cards\_played\_in\_round * 0.2$
\end{center}
In de rewardfunctie zien we ook de term $amount\_of\_cards\_played\_in\_round$ bevat. Ookal zit dit niet in de state gebruiken we dit om acties die vaak leiden tot meer kaarten gespeeld in een ronde extra te belonen.\\\\
Met $\alpha = 0.1$ en $\gamma = 0.75$ (parameters komen uit onderzoek in appendix \ref{appendix:parameters}), behaalt de Q-learning agent volgende resultaten. 
\begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
                \hline
                  vs           & 3 heuristieke spelers & 3 random spelers \\
                \hline
                 Q-learning agent getraind tegen random spelers & 15.1  & 53.2\\
                 Q-learning agent getraind tegen heuristieke spelers & 15.4 & 52.7\\
                \hline
        \end{tabular}
        \caption{W/L in \% voor Q-table agent}
\end{table}
\noindent Dit zijn redelijk goede resultaten, aangezien de random speler een 6\% en 25\% haalt in deze situaties. We hebben dus zeker de random agent overtroffen. Ook tegen de heuristieke speler zien we dat we een redelijke prestatie neerzetten. Het voordeel dat de heuristieke speler heeft is dat hij enkel jokers gebruikt als dat nodig is. De Q-table agent houdt hier geen rekening mee, wat een verklaring kan zijn voor het lagere resultaat.
\begin{figure}[H]
    \centering
    \subfloat[met gesorteerde q-table]{{\includegraphics[width=7.5cm]{images/wins_in_time.png}} }
    \qquad
    \subfloat[alle versies van de q-table tegen random spelers]{{\includegraphics[width=7.5cm]{images/qtable-impl-random.png}} }
    \caption{Resultaten met $\gamma = 0.75$ en $\alpha = 0.1$ doorheen de tijd.}
    \label{fig:example}
\end{figure}
Als we de prestatie van de agent plotten voor meerdere episodes (zie Figuur 2) zien we een opmerkelijk resultaat. Namelijk dat de Q-learning agent de random speler sterk overtreft zelfs zonder training! De oorzaak hiervan ligt bij de implementatie van de Q-table. De acties die voor een state kunnen genomen worden staan in volgorde van laag naar hoog. Als de agent dus een zet moet doen, neemt hij de zet die hij legaal kan doen met de hoogste Q-waarde. Dit wil zeggen dat als de tabel nog maar net geïntialiseerd is (alles op 0), hij de laagste legale zet zal doen die hij kan. Dit op zich is al een kleine heuristiek die beter presteert dan de random keuze van de random agent.\\\\
Om zeker te zijn dat de agent dus wel degelijk goed leert, hebben we ook de Q-table eens omgekeerd geimplementeerd (waar hij dus eerst de hoogst mogelijke kaarten legt) en ook gezorgd dat hij random kiest uit de acties met de hoogste Q-waarden. Na ongeveer 20 000 episodes convergeren deze implementaties, en blijven ze ongeveer stabiel rond de 50\%.

\subsubsection{Uitgebreide state}
Idealiter bevat de state ook de kaarten in je hand. Dit gaat alweer iets meer naar de richting de heuristieke speler, die ook op basis van zijn hand een beslissing maakt. Na wat zorgvuldig rekenwerk zou dit neerkomen op 2872 mogelijke starthanden, nog zonder alle mogelijke states na het leggen van 1,2,.. kaarten. Dit zijn heel veel states! Er moet dus veralgemeend worden, zoals het bijhouden van het aantal kaarten in je hand, en niet de kaarten exact.\\\\
We kunnen de state als volgt voorstellen: (rang, aantal, aantal kaarten in hand). Met de reward functie die hierboven zo goede resultaten gaf, krijgen we nu nog maar rond de 30\% W/L ratio tegen 3 random spelers. Dit is dus niet veel beter meer dan een random agent.\\\\
We zitten hier dus duidelijk met een probleem, namelijk de Q-table die te groot wordt (ook al hebben we die al sterk versimpeld). Hierdoor zijn er veel delen van de Q-table die amper of zelfs nooit worden bezocht, waardoor veel Q-waarden op hun initiele waarde blijven staan, en de agent dus geen goede beslissingen kan maken.\\\\
We zouden de state nog meer moeten veralgemenen. Als we werken met een state\\\\
\indent $(rang, aantal, aantal\_kaarten\_in\_hand)$ met
\begin{center}
$aantal\_kaarten\_in\_hand = aantal$ $if$ $ aantal <= 4$ $else$ $5$
\end{center}
En we initialiseren de state-actie paren op 10 voor de acties op een state die de zorgen dat de speler uitspeelt, gebruikmakend van de rewardfunctie die hierboven beschreven is, behalen we een W/L van 45\% tegen random spelers. Dit is al beter, maar nog steeds niet even goed als de agent met de kleinere Q-table.\\\\
Het feit dat we de state dus niet veel kunnen uitbreiden is een grote beperking van Q-learning. Een te grote Q-table zorgt namelijk voor states die nooit of amper worden tegengekomen in vergelijking met andere states. Ook met de acties is dit het geval, het zal bijvoorbeeld niet veel voorkomen dat je 4 koningen kan leggen waardoor de Q-waarde van deze actie niet veel aangepast gaat worden bij het trainen, en nog minder als deze actie over veel states kan gedaan worden. \\\\
Als we in dit geval positieve rewards geven, en we beginnen met alle state-actie waarden op 0,  dan worden de waarden van de vaak voorkomende state-actie paren groter dan de waarden van zeldzame paren. Echter zijn vaak de zeldzame zetten zeer goed, maar zullen ze niet genomen worden omdat ze een lagere Q-waarde hebben. \\\\
Neem als voorbeeld dat de state bestaat uit $(rang, aantal, aantal\_kaarten\_in\_hand)$ en (3,2,4) is. Onze agent heeft 3 Q'en en een A in zijn hand. De beste zet is de 3 Q'en leggen, waarmee de ronde waarschijnlijk gewonnen zou worden, en dan de A leggen waarmee hij het spel wint. Maar, de actie 3Q zal voor een hand van 4 kaarten hoogstwaarschijnlijk nog niet of nog maar heel weinig zijn voorgekomen waardoor dit een lagere Q-waarde zal hebben dan bijvoorbeeld 1Q leggen.\\\\
We kunnen ook negatieve rewards geven, en alles initialiseren op 0. Stel dat de state hier (4,2,9) is en de agent heeft alle 4 de jokers. Dit komt zelden voor, waardoor de Q-waarde nog een redelijk "hoge" waarde dicht bij 0 zal hebben. Dit betekent dat de agent het leggen van de 4 jokers in 1 zet als een goede actie ziet. Dit is echter niet het geval want dan is hij zijn 4 jokers kwijt.\\\\
TODO deze uitleg beter: Dit is dus een extra groot probleem bij presidenten, aangezien het spel veel states heeft die enkel voorkomen met een beetje geluk. Om zoals in bovenstaand voorbeeld 4 jokers te hebben, zou je deze dus moeten gekregen hebben bij het uitdelen van de kaarten, of een deel bij het uitwisselen tussen president en scum.

\subsection{DQN}
Het doel tot nu toe is het krijgen van gelijkaardige resultaten als bij de heuristieke speler, hierin zijn we nog niet geslaagd. Een mogelijke verklaring is de beperkte state, waarin we niet eens met de volledige hand kunnen rekening houden. Nu we werken met een deep Q-network, is het mogelijk om dit wel te doen.

\subsubsection{Kleine DQN}
We werken hier met een state van 15 velden, de eerste 13 stellen onze hand voor, de laatste 2 representeren de laatst gelegde kaart(en). Dit is even veel informatie als de heuristieke speler ook heeft, wat we tot nu toe nog niet hebben kunnen bereiken op een functionele manier.\\\\
Als eerste testen we de reward functie die ons al veel succes heeft gegeven bij Q-learning. In deze context lijkt die niet zo succesvol te zijn, aangezien die ons nu ongeveer 24\% W/L geeft tegen 3 random spelers. \\\\
Omdat we nu de volledige hand in de state bijhouden, kunnen we misschien een betere reward functie construeren op basis van de hand. We gebruiken het idee van een relatieve hand score, en voegen nog een aantal metrieken toe. De reward functie ziet er als volgt uit in pseudocode:\\\\
\indent als uitgespeeld: return $10$\\
\indent als gepast: return $skip\_penalty$\\
\indent anders: return $relative\_hand\_score$\\\\
De $skip\_penalty$ geeft een hogere straf als er in het begin van de ronde gepast wordt, dan als er op het einde gepast wordt. De $relative\_hand\_score$ geeft een mate van verbetering van de hand aan. Het idee hier achter is dat je zoveel mogelijk je hoge kaarten probeert te behouden (hier is een joker de hoogste rang), en dat alle kaarten leggen van een rang beter is dan minder te leggen. \\\\
We hebben deze functie niet in 1 keer gevonden, maar hebben veel verschillende uitgetest. Het valt van deze korte uitleg misschien niet te merken, maar dit is een heel gefabriceerde en gedetailleerde functie. Daarbij is de relatieve hand score een reward die meer gebaseerd is op het korte termijn gedrag van de agent, het is dus interessant om te kijken hoe de discount factor $\gamma$ hier een rol in speelt.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{images/gamma-iteratie-dqn.png}
\caption{Resultaten variëren van parameter $\gamma$}
\end{figure}\\
We zien inderdaad in Figuur 6, dat bij een hele hoge gamma de agent het zelfs slechter gaat doen dan de random spelers waar hij tegen speelt. In deze context houden we de discount factor dus beter lager dan 0.9.\\\\
Merk op dat we met het construeren van een heel specifieke reward functie, maar een W/L percentage van 45\% behalen tegenover de random spelers. Dit is nog steeds redelijk goed tegen de random spelers, maar zeker nog niet even goed als een heuristieke speler en ook minder goed dan de Q-learning agent met een Q-table. Daarbij is het gedetailleerd fabriceren van een reward functie misschien niet de beste manier van aanpak. De agent volgt namelijk exact wat we aangeven in de reward functie, maar we hebben al vaak gemerkt dat een kleine redeneringsfout de agent kan leiden tot heel vreemde acties.\\\\
Bovendien zijn er grote schommelingen tijdens de training, het netwerk is dus nog vrij onstabiel. Dit zien we ook in figuur 5, waar er soms grote sprongen kunnen zijn die misschien niet te maken hebben met de gamma. Om het netwerk stabieler te maken, hebben we een aantal learning rates uitgetest om te kijken of dit een verschil maakt.\\
\begin{figure}[H]
    \centering
    \subfloat[tegen random spelers]{{\includegraphics[width=7.5cm]{images/lr-iteration-random-small-dqn.png}} }
    \qquad
    \subfloat[tegen heuristieke spelers]{{\includegraphics[width=7.5cm]{images/lr-iteration-heur-small-dqn.png}} }
    \caption{Resultaten met $\gamma = 0.5$ en $\alpha = \{0.01, 0.001, 0.0001\}$ doorheen de tijd.}
    \label{fig:example}
\end{figure}
Een lagere learning rate lijkt het netwerk dus inderdaad stabieler te maken, en behoudt de W/L redelijk hoog. Met een hogere learning rate behalen we vaak slechtere resultaten en zien we grote schommelingen. Een learning rate van 0.0001 is hier dus beter. Op deze grafieken zien we ook dat na 20 000 episodes, het netwerk geconvergeerd lijkt te zijn en niet veel meer veranderd. \\\\
\begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
                \hline
                  vs           & 3 heuristieke spelers & 3 random spelers \\
                \hline
                 DQN agent getraind tegen random spelers & 9.8 & 39.5\\
                 DQN agent getraind tegen heuristieke spelers & 11.3 & 39.9\\
                \hline
        \end{tabular}
        \caption{W/L in \% voor DQN agent}
\end{table}
Nu we een goede learning rate en discount factor hebben gevonden, kunnen we de gemiddelde scores die dit netwerk behaald vergelijken. Merk op dat de agent die traint tegen de heuristieke spelers, gemiddeld iets hogere scores haalt tegen de heuristieke spelers. \\
\subsubsection{Grote DQN}
In het vorige deel hebben we een vrij gedetailleerde reward functie gemaakt, die elke stap een reward geeft. We willen echter de agent minder zelf sturen, en zelf de taktiek van het spel laten ontdekken. Daarvoor gebruiken we volgende, simpele reward functie:\\\\
\indent als president return $10$\\
\indent als president return $5$\\
\indent als president return $-5$\\
\indent als president return $-10 - 3*amount\_cards\_in\_hand$\\
\indent anders return 0\\

\noindent De agent krijgt dus enkel een reward op het einde van het spel, en krijgt voor alle andere stappen een 0. De state houdt vanaf nu ook alle gelegde kaarten bij in het spel. Er zijn dus 13 velden voor de eigen hand, 13 velden (op dezelfde manier als eigen hand) voor de gelegde kaarten in het spel, en 2 voor de laatste gelegde kaart(en). Aangezien we nu enkel rewards geven voor de finale actie van een spel vermoeden we dat een hogere gamma voor betere resultaten zal zorgen.\\
\begin{figure}[H]
    \centering
    \subfloat[tegen random spelers]{{\includegraphics[width=7.5cm]{images/gamma_bigdqn_20k.png}} }
    \qquad
    \subfloat[tegen heuristieke spelers]{{\includegraphics[width=7.5cm]{images/gamma_bigdqn_100k.png}} }
    \caption{Resultaten variëren van $\gamma$.}
\end{figure}
\noindent Als we voor verschillende waarden van $\gamma$ de prestatie van het netwerk simuleren zien we dat voor iets hogere gammas veel betere resultaten worden gehaald. Maar de gamma mag niet te hoog zijn. We zien ook dat we weer grote schommelingen zien in de prestatie. Daarom hebben we ook eens de invloed van de learningrate parameter bekeken. 
\begin{figure}[H]
    \centering
    \subfloat[tegen random spelers]{{\includegraphics[width=7.5cm]{images/lr-iteration-random-big-dqn.png}} }
    \qquad
    \subfloat[tegen heuristieke spelers]{{\includegraphics[width=7.5cm]{images/lr-iteration-heur-big-dqn.png}} }
    \caption{Resultaten met $\gamma = 0.7$ en $\alpha = \{0.01, 0.001, 0.0001\}$ doorheen de tijd.}
\end{figure}
\noindent Net als bij de kleine DQN zien we opnieuw grote bewegingne in de prestatie. Hier is er wel geen learningratefactor die een betere stabiliteit biedt. De oorzaak van deze grote schommelingen is misschien omdat er nog niet genoeg getraind is, de state is groter dan de kleine DQN maar het aantal trainingen is gelijk gebleven. Om te kijken of de stabiliteit van het model zou verbeteren hebben we het netwerk met $ lr = 0.0001$ langer laten trainen.
\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{images/big_dqn_extended_training.png}
\caption{Resulaten langere training met $lr = 0.0001, \gamma = 0.7$}
\end{figure}
\noindent We zien dat ook als de training langer is het netwerk zeer onstabiel blijft. Dit is dan ook de reden waarom we voor de grote DQN geen besluitende prestatieresultaten gaan geven. We zouden hier aan cherry picking kunnen doen en besluiten dat ons netwerk 42\% tegen random spelers behaalt dit zou echter niet zeer waardevol zijn aangezien het goed mogelijk is dat 20000 trainingen later de W/L maar 5\% meer bedraagt.
\section{conclusie}
- simpelere rewards die de agent meer zijn ding laten doen lijken het beter te doen
- zelfs simpelere algoritmen lijken het beter te doen\\\\

Ook interessant om zeker over te praten:\\
https://www.alexirpan.com/2018/02/14/rl-hard.html\\
Veel van de observaties in de blogpost komen overeen wat we hier hebben gemerkt zoals\\
- vinden van een reward functie is niet moeilijk, maar het vinden van de beste reward functie wel\\
- Een reward functie kan duidelijk een slechte invloed hebben op de agent aangezien die exact doet wat wij specifieren in de reward functie, zo kunnen we dus raar gedrag krijgen waar de agents opeens blijven skippen etc.
- overfitten kan ook een probleem zijn bij DQN, zeker in de case van presidenten (zie bovenstaande uitleg van thor)
- nog zo veel mogelijke problemen: states die niet goed zijn voorgesteld, foute parameters, slechte netwerk structuur, ...

\newpage
\begin{thebibliography}{9}
\bibitem{simple-qlearning} 
Reinforcement learning tutorial using Python and Keras 
\\\texttt{https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/}

\bibitem{nn-paper} 
Application of Self-Play Reinforcement Learning to a Four-Player Game of Imperfect Information
\\\texttt{https://arxiv.org/pdf/1808.10442.pdf}

\bibitem{mct-1} 
Learning in POMDPs with Monte Carlo Tree Search
\\\texttt{http://proceedings.mlr.press/v70/katt17a/katt17a.pdf}

\bibitem{mct-2} 
todo
\\\texttt{https://fse.studenttheses.ub.rug.nl/15440/1/Bachelor\_Thesis\_-\_Maxiem\_Wagen\_1.pdf}

\bibitem{mct-3} 
todo
\\\texttt{https://www.researchgate.net/publication/337508729\_Reinforcement\_Learning\_in\_Card\_Game\_Environments\_Using\_Monte\_Carlo\_Methods\_and\_Artificial\_Neural\_Networks}

\bibitem{bellman-equations} 
Bellman Equations
\\\texttt{https://en.wikipedia.org/wiki/Bellman\_equation}
\end{thebibliography}
\newpage


\appendix
\section{\\Studie learning rate en discount factor voor state met 2 velden}
% the \\ insures the section title is centered below the phrase: AppendixA
\label{appendix:parameters}
Voor de juiste learning rate en discount factor kunnen we simulaties laten lopen om zo de optimale parameters te vinden. Hieronder zien we de W/L voor alle mogelijke parameters voor $\alpha$ en $\gamma$ met stappen van $0.1$.
\begin{figure}[h]
\centering
\includegraphics[scale=0.50]{images/qtable_parameter_random.png}
\includegraphics[scale=0.50]{images/qtable_parameter_heuristic.png}
\caption{Resultaten variëren met parameters $\gamma$ en $\alpha$ na 50000 episodes, links tegen random, rechts tegen heuristieke spelers}
\end{figure}
We zien nu dat er voor $\gamma \in [0.5, 0.8] $ en $\alpha \in [0.1, 0.3] $ meer winst wordt behaald. Bovenstaande figuren zijn bekomen met training van 50000 keer. We zullen nu dubbel zo veel trainen voor deze intervallen om zo de optimale $\gamma$ en $\alpha$ te vinden\\\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.50]{images/qtable_parameter_random_zoomed.png}
\includegraphics[scale=0.50]{images/qtable_parameter_heuristic_zoomed.png}
\caption{Resultaten variëren met parameters $\gamma$ en $\alpha$ na 100000 episodes, links tegen random, rechts tegen heuristieke spelers}
\end{figure}\\
Uit bovenstaande afbeeldingen kunnen we besluiten dat de optimale parameters, voor de gebruikte rewardfunctie, $\alpha = 0.1$ en $\gamma = 0.75$ zijn.

\section{\\Epsilon-decay TODO}
\label{appendix:epsilon}
Aangezien we met $\epsilon$ decay werken, illustreren we hieronder dat het belangrijk is om je $epsilon\_decay$ factor dicht genoeg bij 1 te nemen. Anders zal $\epsilon$ al snel zijn minimum hebben bereikt en zal er misschien niet genoeg exploratie geweest zijn.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/epsilon_decay.png}
\caption{Illustratie van epsilondecay met een minimum $\epsilon$ van 0.05}
\end{figure}\\
Voor ons model kozen wij voor een $epsilon\_decay$ factor van 0.9999. Als we 100000 episodes trainen, bereiken we de minimale epsilon dan na ongeveer 30000 episodes. Dat betekent dat er na 30\% van de episodes voornamelijk exploitatie is. Het blijkt echter dat ons model al na 30000 episodes geconvergeerd is. Daarom bekijken we of een kleinere $epsilon\_decay$ factor niet betere resultaten oplevert.\\\\
In onderstaande tabel staan de resultaten van simulaties gedaan voor de $epsilon\_decay$ factoren uit bovenstaande grafiek, voor een model met $\alpha = 0.1$ en $\gamma = 0.75$ (\ref{appendix:parameters})
\begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
                \hline
                                & 0.99&0.999&0.9999&0.99999\\
                                \hline
                3 random speler  &52.83&50.65&51.1&52.41\\
                3 heuristieke spelers      &14.77&14.87&14.49&15.12\\
                \hline
        \end{tabular}
        \caption{W/L voor Q-table agent in \% voor verschillende epsilondecay factoren na 100000 episodes}
\end{table}
\noindent We zien dat zelfs voor een $epsilon\_decay$ factor van 0.99 de agent even goed presteert als onze initiele agent die met 0.9999 is getraind. Ook voor $epsilon\_decay = 0.99999$, waarbij er nog veel exploratie is na 100000 episodes, is dit het geval. We kunnen dus besluiten dat de $\epsilon$ en $epsilon\_decay$ parameters een beperkte invloed hebben op de prestatie van de agent.\\\\

\end{document}
