\documentclass[11pt]{article}
\usepackage{a4wide}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\begin{titlepage}
\title{Reinforcement learning gebaseerde agent voor presidenten}
\author{Freya Van Speybroek \& Thor Dossche}
\date{Academiejaar 2020 - 2021}
\maketitle
\thispagestyle{empty}
\end{titlepage}


\section{Inleiding}
In dit project zullen we bestuderen hoe reinforcement learning kan toegepast worden op een kaartspel. Het kaartspel dat we gaan bekijken is presidenten. Dit is een perfect voorbeeld van een spel waarbij we incomplete informatie hebben: er kan namelijk niet in de kaarten van de tegenstanders gekeken worden. Daarbij is elke toestand discreet, waardoor het probleem kan gemodelleerd worden als een Partially Observable Markov Decision Process (POMDP). \\\\ Voor dit soort problemen zijn er al heel wat oplossingsmethodes, waarvan we er een paar zullen uitproberen en vergelijken. Daarbij kunnen we ons ook de vraag stellen of het wel degelijk gunt om een Reinforcement Learning algoritme te gebruiken. Zou het kunnen dat minder complexe oplossingen ons betere resultaten bieden?
\subsection{Het probleem}
Presidenten kent veel varianten, de regels liggen namelijk niet hard vast en kunnen daarom vaak besproken worden onder de spelers. Afhankelijk van welke regels je gebruikt, kan het spel moeilijker worden. \\\\Er zijn wel een paar vaste regels die de meeste spelers volgen:\\\\
- De speler die de ‘klaver 3’ heeft, mag als eerste leggen in de ronde. 
- Spelers moeten altijd een kaart leggen die hoger of gelijk aan de vorige kaart is. Het aantal van die kaart moet hoger of gelijk zijn aan de kaart(en) van de vorige beurt.
- Als een speler uit is, dan wordt een nieuwe ronde gestart en mag de speler na de net uitgespeelde speler beginnen.
- Als iedereen gepast heeft, dan begint er een nieuwe ronde en mag de speler die het laatst een kaart heeft gelegd beginnen.
- Een speler mag altijd passen, maar moet passen als hij niet meer kan leggen. In het geval van een pas, mag de speler de volledige ronde niet meer meespelen.
- Vanaf een ronde gedaan is, en er zijn minstens 2 spelers nog niet uitgespeeld, dan begint er een nieuwe ronde. Alle spelers die vorige ronden hebben gepast, mogen nu terug meedoen.
- De persoon die het eerste uitspeelt is de president, de tweede de vice-president. 
- De persoon die als laatste overblijft is de scum, degene die als voorlaatste uitspeelt is de vice-scum.
- De president krijgt de beste 2 kaarten van de scum, de scum krijgt de 2 slechtste van de president. Hetzelfde geldt voor vice-president en vice-scum, maar die wisselen slechts 1 kaart uit.\\\\
Om het spel interessanter te maken hebben wij nog een paar extra regels toegevoegd:

- Als een 7 is gespeeld, dan moet de volgende kaart lager of gelijk zijn aan die 7.
- De kaart 2 is een joker die kan gebruikt worden als hoogste kaart in het spel, of als een verdubbelingskaart als die gelegd wordt samen met een kaart van een andere waarde.
- De ranking is dus als volgt: 3,4,5,6,7,8,9,10,V,Q,K,1,2
\\\\
Merk op dat we het hier hebben over een spel met verschillende rondes. De winnaar van het spel wordt president, maar de winnaar van de ronde wordt dit niet. 
\subsection{De data}
Bij Reinforcement Learning gaan we niet aan de slag met een dataset, maar met een “environment”, die het spel zelf voorstelt. We willen dan een agent bekomen die een zo hoog mogelijke score behaalt in de environment. \\\\ In dit geval, willen we een agent die zo vaak mogelijk of zo lang mogelijk president kan zijn. Maar, de situatie is niet zo zwart-wit. Het kan namelijk ook goed zijn om veel vice-president te zijn, of om met heel slechte kaarten - en dus met wat ongeluk - toch geen scum te worden. Zou zelfs nooit scum worden kunnen gezien worden als een goed resultaat?  Het is niet zo dat 1 uitkomst goed is, en alle andere slecht. \\\\ Bovendien kan de environment ook op veel verschillende manieren bekeken worden. Hoe wordt een state voorgesteld, welke rewards delen we uit, welke acties maken we mogelijk? Dit zijn allemaal deelproblemen op zich, waarvoor we verschillende oplossingen zullen proberen vinden.
\subsection{Mogelijke oplossingsmethoden}
\section{xxx}
\subsection{Q-Table}
\subsection{Deep Q Learning}

\end{document}